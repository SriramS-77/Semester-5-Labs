{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b85082-a633-4d4c-9e34-d792b3e9521d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "! python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47ae420-53cf-4b8b-97ab-2c15695020e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, explode, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf9c0087-f05a-45f5-add3-51f334285851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/30 09:48:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Hadoop').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2874beae-683c-4d61-81fc-ae2245039121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n",
      "|_c0|_c1| _c2|_c3|_c4| _c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|   _c41|\n",
      "+---+---+----+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n",
      "|  0|tcp|http| SF|181|5450|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   8|   8| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   9|   9| 1.0| 0.0|0.11| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n",
      "+---+---+----+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('inferSchema', True).option('header', False).csv('kddcup.data_10_percent_corrected')\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c26341d2-3c0f-46cd-9010-078e57a11ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: integer (nullable = true)\n",
      " |-- _c15: integer (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: integer (nullable = true)\n",
      " |-- _c20: integer (nullable = true)\n",
      " |-- _c21: integer (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: integer (nullable = true)\n",
      " |-- _c24: double (nullable = true)\n",
      " |-- _c25: double (nullable = true)\n",
      " |-- _c26: double (nullable = true)\n",
      " |-- _c27: double (nullable = true)\n",
      " |-- _c28: double (nullable = true)\n",
      " |-- _c29: double (nullable = true)\n",
      " |-- _c30: double (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: integer (nullable = true)\n",
      " |-- _c33: double (nullable = true)\n",
      " |-- _c34: double (nullable = true)\n",
      " |-- _c35: double (nullable = true)\n",
      " |-- _c36: double (nullable = true)\n",
      " |-- _c37: double (nullable = true)\n",
      " |-- _c38: double (nullable = true)\n",
      " |-- _c39: double (nullable = true)\n",
      " |-- _c40: double (nullable = true)\n",
      " |-- _c41: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ddeeed6-562b-4339-97c1-cbeb96e93cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('_c1', '_c2', '_c3').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3175472d-b6e8-41e9-b8b0-7bf46ae6fa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=df.columns[:-1], outputCol='featureVector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2872ba30-8a4c-4c15-85b4-ae1f518ae800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "kmeans = KMeans(predictionCol='cluster', featuresCol='featureVector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3088ecd1-b02e-4e11-beab-f0670a955267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 09:57:25 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "[Stage 22:===============================>                        (10 + 8) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([4.79793956e+01, 1.62207883e+03, 8.68534183e+02, 4.45326100e-05,\n",
      "       6.43293794e-03, 1.41694668e-05, 3.45168212e-02, 1.51815716e-04,\n",
      "       1.48247035e-01, 1.02121372e-02, 1.11331525e-04, 3.64357718e-05,\n",
      "       1.13517671e-02, 1.08295211e-03, 1.09307315e-04, 1.00805635e-03,\n",
      "       0.00000000e+00, 0.00000000e+00, 1.38658354e-03, 3.32286248e+02,\n",
      "       2.92907143e+02, 1.76685418e-01, 1.76607809e-01, 5.74330999e-02,\n",
      "       5.77183920e-02, 7.91548844e-01, 2.09816404e-02, 2.89968625e-02,\n",
      "       2.32470732e+02, 1.88666046e+02, 7.53781203e-01, 3.09056111e-02,\n",
      "       6.01935529e-01, 6.68351484e-03, 1.76753957e-01, 1.76441622e-01,\n",
      "       5.81176268e-02, 5.74111170e-02]), array([2.0000000e+00, 6.9337564e+08, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
      "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.7000000e+01,\n",
      "       3.0000000e+00, 7.9000000e-01, 6.7000000e-01, 2.1000000e-01,\n",
      "       3.3000000e-01, 5.0000000e-02, 3.9000000e-01, 0.0000000e+00,\n",
      "       2.5500000e+02, 3.0000000e+00, 1.0000000e-02, 9.0000000e-02,\n",
      "       2.2000000e-01, 0.0000000e+00, 1.8000000e-01, 6.7000000e-01,\n",
      "       5.0000000e-02, 3.3000000e-01])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "pipeline_model = pipeline.fit(df)\n",
    "\n",
    "kmeans_model = pipeline_model.stages[1]\n",
    "\n",
    "print(kmeans_model.clusterCenters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "722fb0bb-1151-4243-8c1e-de523ee52197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|cluster|\n",
      "+-------+\n",
      "|      0|\n",
      "|      0|\n",
      "|      0|\n",
      "+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_cluster = pipeline_model.transform(df)\n",
    "\n",
    "df_with_cluster.select('cluster').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf59358-9596-43fc-8ab2-cc2cc50f5710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
