{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c6c1be4-e4a2-405f-9865-50564de21931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "! python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c819b3-5d75-497f-8363-5ea43de60c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6434459-5a8d-4839-b652-d8a16f31a2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/09 09:08:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkOperations').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f55be27c-e3cf-4c9a-bd5a-86ed5cb411ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "| _c0|_c1|_c2|_c3|_c4|_c5|_c6|_c7|_c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|_c41|_c42|_c43|_c44|_c45|_c46|_c47|_c48|_c49|_c50|_c51|_c52|_c53|_c54|\n",
      "+----+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|2596| 51|  3|258|  0|510|221|232|148|6279|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5|\n",
      "+----+---+---+---+---+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('inferSchema', True).option('header', False).csv('covtype.data')\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "839b3596-c9f8-4e60-b89c-d1fdd92103f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: integer (nullable = true)\n",
      " |-- _c15: integer (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: integer (nullable = true)\n",
      " |-- _c20: integer (nullable = true)\n",
      " |-- _c21: integer (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: integer (nullable = true)\n",
      " |-- _c24: integer (nullable = true)\n",
      " |-- _c25: integer (nullable = true)\n",
      " |-- _c26: integer (nullable = true)\n",
      " |-- _c27: integer (nullable = true)\n",
      " |-- _c28: integer (nullable = true)\n",
      " |-- _c29: integer (nullable = true)\n",
      " |-- _c30: integer (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: integer (nullable = true)\n",
      " |-- _c33: integer (nullable = true)\n",
      " |-- _c34: integer (nullable = true)\n",
      " |-- _c35: integer (nullable = true)\n",
      " |-- _c36: integer (nullable = true)\n",
      " |-- _c37: integer (nullable = true)\n",
      " |-- _c38: integer (nullable = true)\n",
      " |-- _c39: integer (nullable = true)\n",
      " |-- _c40: integer (nullable = true)\n",
      " |-- _c41: integer (nullable = true)\n",
      " |-- _c42: integer (nullable = true)\n",
      " |-- _c43: integer (nullable = true)\n",
      " |-- _c44: integer (nullable = true)\n",
      " |-- _c45: integer (nullable = true)\n",
      " |-- _c46: integer (nullable = true)\n",
      " |-- _c47: integer (nullable = true)\n",
      " |-- _c48: integer (nullable = true)\n",
      " |-- _c49: integer (nullable = true)\n",
      " |-- _c50: integer (nullable = true)\n",
      " |-- _c51: integer (nullable = true)\n",
      " |-- _c52: integer (nullable = true)\n",
      " |-- _c53: integer (nullable = true)\n",
      " |-- _c54: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93a56242-c9a9-47cd-90ca-e81182396237",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([.9, .1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c45f88a8-5e01-47b1-8357-774061ab0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afb4c3f0-5463-4d03-acef-42d7c01c12b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                        |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1879.0,28.0,19.0,30.0,12.0,95.0,209.0,196.0,117.0,778.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1888.0,33.0,22.0,150.0,46.0,108.0,209.0,185.0,103.0,735.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1889.0,28.0,22.0,150.0,23.0,120.0,205.0,185.0,108.0,759.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1889.0,353.0,30.0,95.0,39.0,67.0,153.0,172.0,146.0,600.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1896.0,337.0,12.0,30.0,6.0,175.0,195.0,224.0,168.0,732.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1898.0,34.0,23.0,175.0,56.0,134.0,210.0,184.0,99.0,765.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1899.0,355.0,22.0,153.0,43.0,124.0,178.0,195.0,151.0,819.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1901.0,311.0,9.0,30.0,2.0,190.0,195.0,234.0,179.0,726.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1903.0,5.0,13.0,42.0,4.0,201.0,203.0,214.0,148.0,708.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1903.0,67.0,16.0,108.0,36.0,120.0,234.0,207.0,100.0,969.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1904.0,51.0,26.0,67.0,30.0,162.0,222.0,175.0,72.0,711.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1905.0,19.0,27.0,134.0,58.0,120.0,188.0,171.0,108.0,636.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1905.0,77.0,21.0,90.0,38.0,120.0,241.0,196.0,75.0,1025.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1916.0,24.0,25.0,212.0,74.0,175.0,197.0,177.0,105.0,789.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1916.0,320.0,24.0,190.0,60.0,162.0,151.0,210.0,195.0,832.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1919.0,30.0,22.0,67.0,9.0,256.0,208.0,188.0,107.0,661.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1919.0,44.0,26.0,162.0,68.0,150.0,216.0,173.0,77.0,706.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1922.0,50.0,29.0,108.0,43.0,218.0,219.0,165.0,62.0,659.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1925.0,352.0,22.0,210.0,74.0,182.0,176.0,197.0,155.0,779.0,1.0,1.0])|\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "input_cols = cols[:-1]\n",
    "vec_ass = VectorAssembler(inputCols=input_cols,\n",
    "                         outputCol=\"featureVector\")\n",
    "assembled_train_data = vec_ass.transform(train_data)\n",
    "assembled_train_data.select(\"featureVector\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7106f5da-6bf6-469b-99c4-802d287ca6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_021026b8aba8, depth=5, numNodes=39, numClasses=8, numFeatures=54\n",
      "  If (feature 0 <= 3047.5)\n",
      "   If (feature 0 <= 2501.5)\n",
      "    If (feature 3 <= 15.0)\n",
      "     If (feature 12 <= 0.5)\n",
      "      If (feature 23 <= 0.5)\n",
      "       Predict: 4.0\n",
      "      Else (feature 23 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 12 > 0.5)\n",
      "      Predict: 6.0\n",
      "    Else (feature 3 > 15.0)\n",
      "     If (feature 16 <= 0.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 16 > 0.5)\n",
      "      If (feature 9 <= 1335.0)\n",
      "       Predict: 3.0\n",
      "      Else (feature 9 > 1335.0)\n",
      "       Predict: 4.0\n",
      "   Else (feature 0 > 2501.5)\n",
      "    If (feature 17 <= 0.5)\n",
      "     If (feature 15 <= 0.5)\n",
      "      Predict: 2.0\n",
      "     Else (feature 15 > 0.5)\n",
      "      Predict: 3.0\n",
      "    Else (feature 17 > 0.5)\n",
      "     If (feature 0 <= 2711.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 0 > 2711.5)\n",
      "      If (feature 5 <= 1259.5)\n",
      "       Predict: 5.0\n",
      "      Else (feature 5 > 1259.5)\n",
      "       Predict: 2.0\n",
      "  Else (feature 0 > 3047.5)\n",
      "   If (feature 0 <= 3310.5)\n",
      "    If (feature 7 <= 240.5)\n",
      "     Predict: 1.0\n",
      "    Else (feature 7 > 240.5)\n",
      "     If (feature 3 <= 330.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 330.5)\n",
      "      Predict: 2.0\n",
      "   Else (feature 0 > 3310.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     If (feature 3 <= 284.0)\n",
      "      If (feature 6 <= 207.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 6 > 207.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 3 > 284.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 926.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 926.5)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(seed=1234,\n",
    "                                   labelCol=\"_c54\",\n",
    "                                   featuresCol=\"featureVector\",\n",
    "                                   predictionCol=\"prediction\")\n",
    "\n",
    "model = classifier.fit(assembled_train_data)\n",
    "\n",
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76b2a5ed-4d04-48de-9d51-d0ee30c6a165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                       (0 + 18) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "assembled_test_data = vec_ass.transform(test_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(assembled_test_data)\n",
    "\n",
    "# Evaluate the model performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"_c54\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
